
# initial setup
library(dplyr)
library(glmnet)
library(caret)
library(nnet)
library(randomForest)
library(naivebayes)
library(ggplot2)
library(corrplot)
library(pROC)
set.seed(0802)


# upload data, clean data
training_outcomes = read.csv("ml.csv",sep = ";", na.strings = "", stringsAsFactors = T)
training_outcomes = training_outcomes[, c(1, 2, 4:19, 22:32, 34:36,33)]
str(training_outcomes)

training_outcomes$highest_status = factor(training_outcomes$highest_status, levels = c("Discontinued", 
                                                                                       "Pre Clinical Trials", 
                                                                                       "Phase 1 Clinical Trials", 
                                                                                       "Phase 2 Clinical Trials",  
                                                                                       "Phase 3 Clinical Trials", 
                                                                                       "Approved")) 

# outcome visualization
          # create a color palette for highest_status
ggplot(data = training_outcomes, aes(x = highest_status, fill = highest_status)) + 
    geom_bar() + 
    labs(title = "Highest status in drug development pipeline") +
    theme(plot.title = element_text(size = 12, face = "bold"),
          panel.background = element_rect(fill = "transparent"),
          panel.grid.major = element_blank(),                   #
          panel.grid.minor = element_blank()) +
          geom_text(stat = "count", aes(label = stat(count)), vjust = -0.5) + 
          theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(data = training_outcomes, aes(x = highest_status)) + 
  geom_bar() + 
  labs(title = "Highest status in drug development pipeline") +
  theme(plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_rect(fill = "transparent"),
        panel.grid.major = element_blank(),                   #
        panel.grid.minor = element_blank()) +
  geom_text(stat = "count", aes(label = stat(count)), vjust = -0.5) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# (3:11) is integer, (12:16) is numeric, (17:32) is categorical
# numeric variables correlation
cor = cor(training_outcomes[,3:16], use = "complete.obs")
corrplot(cor,
         type = "upper",
         tl.col = "black",
         tl.cex = 0.8,
         tl.srt = 45,
         title = "Correlation table for numerical features")





#data partition
training_outcomes_idx = createDataPartition (training_outcomes$highest_status, 
                                             p = 0.8, list = F) #assume 80% will be training set






#m0: logistic regression with all features except str value (inchikey and smiles)
          ## check feature importance in the 
m0_train = training_outcomes[training_outcomes_idx, c(3:33)]
m0_test = training_outcomes[-training_outcomes_idx, c(3:33)]

m0 <- multinom(highest_status ~ ., data = m0_train, family = multinomial)

coef(m0)

m0_ci = confint(m0, level=0.95) 
m0_ci[1]
names(m0_ci)

#check feature importance
a = coef(m0) %>% 
  abs() %>% 
  apply(1,function(x)rank(-x)) %>% 
  t()

#feature importance for each class
for (i in c(1:10)) {
  b = colnames(a)[a[5,] == i]
  print(b)
}



m0_feature_importance = colMeans(a) %>% 
           sort() %>% 
            head(30) %>% 
              as.data.frame

#m0 prediction
pre_m0 <- predict(m0, newdata = m0_test)
cm = confusionMatrix(data = pre_m0, reference = m0_test$highest_status)
m0_cm = cm$table 

#m0 result
cm$overall
      #prop.test(sum(diag(m0_cm)), sum(m0_cm))$conf.int #accuracy CI

# accuracy, precision, sensitivity, f1 for each class and all, AUC ROC for all
accuracy <- 1
balanced_accuracy <- 1
precision <- 1
sensitivity <- 1
f1_score <- 1
i=2
(m0_cm[i,i] + (sum(m0_cm) - sum(m0_cm[i,]) - sum(m0_cm[,i]) + m0_cm[i,i])) /
  sum(m0_cm)

for (i in 1:nrow(m0_cm)) {
  accuracy[i] = sum(diag(m0_cm)) / sum(m0_cm)
  balanced_accuracy[i] = ((m0_cm[i, i] / sum(m0_cm[, i])) + (sum(m0_cm[-i, -i]) / sum(m0_cm[-i, ]))) / 2
  precision[i] = m0_cm[i,i] / sum(m0_cm[i,])
  sensitivity[i] = m0_cm[i,i] / sum(m0_cm[,i])
  f1_score[i] <- 2 * (precision[i] * sensitivity[i]) / (precision[i] + sensitivity[i])
}
m0_metrics <- data.frame(
  Outcome = colnames(m0_cm),
  Accuracy = accuracy,
  Balanced_accuracy = balanced_accuracy, 
  Precision = precision,
  Sensitivity = sensitivity,
  F1_Score = f1_score
) %>% 
  rbind(c(
    "Overall",
    mean(accuracy),
    mean(balanced_accuracy),
    mean(precision),
    mean(sensitivity),
    mean(f1_score)
    ))
m0_metrics$AUC_ROC[7] = 0.6515

                # calculate roc
                roc = multiclass.roc(m0_test$highest_status, factor(pre_m0, levels = c("Discontinued", 
                                                                                 "Pre Clinical Trials", 
                                                                                 "Phase 1 Clinical Trials", 
                                                                                 "Phase 2 Clinical Trials",  
                                                                                 "Phase 3 Clinical Trials", 
                                                                                 "Approved"),
                                                              , ordered = T)) 
                for (i in 1:15) {
                  plot(roc$rocs[[i]], col = "red", main = "Multiclass ROC Curve")  
                }
                
                roc$rocs[[15]]
                str(roc)

m0_metrics$Outcome = factor(m0_metrics$Outcome, levels = c("Discontinued", 
                                                                                       "Pre Clinical Trials", 
                                                                                       "Phase 1 Clinical Trials", 
                                                                                       "Phase 2 Clinical Trials",  
                                                                                       "Phase 3 Clinical Trials", 
                                                                                       "Approved", 
                                           "Overall")) 


for (i in 2:6) 
  {
  plot = 
    ggplot(data = m0_metrics, aes(x = Outcome, y = as.numeric(m0_metrics[, i])), fill = Outcome) + 
    geom_col() + 
    geom_hline(yintercept = mean(as.numeric(m0_metrics[, i])), color = "red", linetype = "dashed", 
               size = 1, show.legend = T) +
    theme(plot.title = element_text(size = 12, face = "bold"),
          panel.background = element_rect(fill = "transparent")) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    labs(title = colnames(m0_metrics)[i], x = "", y = "")
  print(plot)
}

# add standard deviation

  rbind(m0_metrics, c("Standard Deviation", apply(m0_metrics[, -1], 2, sd)))


              



     
#kfold 
k = 10
folds <- createFolds(training_outcomes$highest_status, k = k)

for (fold in 1:k) {
  train_indices <- unlist(folds[-fold])
  test_indices <- folds[[fold]]
  train_data <- training_outcomes[train_indices, ]
  test_data <- training_outcomes[test_indices, ]
  
  multinom_model <- multinom(highest_status ~ ., data = training_outcomes_ml_train, family = multinomial)
  
  predictions_multinom <- predict(multinom_model, newdata = training_outcomes_ml_test)
  
  cm_multinom <- table(training_outcomes_ml_test$highest_status, predictions_multinom)
  accuracy_multinom <- sum(diag(cm_multinom)) / sum(cm_multinom)
  print(accuracy_multinom)
}








#lasso regression for data regulation
#lamda grid

## lasso grid 1
grid_lasso1 = expand.grid(alpha = 1, lambda = 10^seq(0,2,length = 100))
m_lasso1 <- train(highest_status ~., 
                 data = training_outcomes[3:33],
                 method = "glmnet",
                 trControl = trainControl(method = "cv", number = 10),
                 tuneGrid = grid_lasso1)
#result of m_lasso
coef(m_lasso1$finalModel, m_lasso1$bestTune$lambda)

cbind(a1[[1]], a1[[2]], a1[[3]], a1[[4]], a1[[5]], a1[[6]])

#conclusion: a small lambda suggests multiple variables are relevant for prediction
a1 = coef(m_lasso1$finalModel, m_lasso1$bestTune$lambda) 
b1 = do.call(cbind, a1) 
colnames(b1) = names(a1)
#select the 10 most important features for each outcome
c1 = apply(b1, 2,
          function(x)
            names(x[order(abs(x), decreasing = T)[1:11]])) %>%
  as.data.frame()
m1_feature_importance1 = unlist(c1) %>% 
  table() %>% 
  sort(decreasing = T)

## lasso grid 2
grid_lasso2 = expand.grid(alpha = 1, lambda = 10^seq(0,-2,length = 100))
m_lasso2 <- train(highest_status ~., 
                 data = training_outcomes[3:33],
                 method = "glmnet",
                 trControl = trainControl(method = "cv", number = 10),
                 tuneGrid = grid_lasso2)
#result of m_lasso
coef(m_lasso2$finalModel, m_lasso2$bestTune$lambda)
#conclusion: a small lambda suggests multiple variables are relevant for prediction
a2 = coef(m_lasso2$finalModel, m_lasso2$bestTune$lambda) 
b2 = do.call(cbind, a2) 
colnames(b2) = names(a2)

cbind(a2[[1]], a2[[2]], a2[[3]], a2[[4]], a2[[5]], a2[[6]])

#select the 10 most important features for each outcome
c2 = apply(b2, 2,
          function(x)
            names(x[order(abs(x), decreasing = T)[1:11]])) %>%
  as.data.frame()
m1_feature_importance2 = unlist(c2) %>% 
  table() %>% 
  sort(decreasing = T)

## lasso grid 3
grid_lasso3 = expand.grid(alpha = 1, lambda = 10^seq(-2,-4,length = 100))
m_lasso3 <- train(highest_status ~., 
                 data = training_outcomes[3:33],
                 method = "glmnet",
                 trControl = trainControl(method = "cv", number = 10),
                 tuneGrid = grid_lasso3)
#result of m_lasso
coef(m_lasso3$finalModel, m_lasso3$bestTune$lambda)
#conclusion: a small lambda suggests multiple variables are relevant for prediction
a3 = coef(m_lasso3$finalModel, m_lasso3$bestTune$lambda) 
b3 = do.call(cbind, a3) 
colnames(b3) = names(a3)
#select the 10 most important features for each outcome
c3 = apply(b3, 2,
          function(x)
            names(x[order(abs(x), decreasing = T)[1:11]])) %>%
  as.data.frame()
m1_feature_importance3 = unlist(c) %>% 
  table() %>% 
  sort(decreasing = T)







# random forest & artificial neural network 
training_outcomes <- read.csv("ml.csv",sep = ";", header = T)

# Removing the string columns and other non-relevant columns while completing other data imputation steps (Ex: defining factor variables): 

```{r, warning = FALSE}
training_outcomes_ml <- training_outcomes[, -c(1, 2, 3, 20, 21)]

training_outcomes_ml$ro3_pass <- factor(training_outcomes_ml$ro3_pass)
training_outcomes_ml$molecular_species <- factor(training_outcomes_ml$molecular_species)
training_outcomes_ml$molecule_type <- factor(training_outcomes_ml$molecule_type)
training_outcomes_ml$metabolism_study <- factor(training_outcomes_ml$metabolism_study)
training_outcomes_ml$metabolism_enzyme <- factor(training_outcomes_ml$metabolism_enzyme)
training_outcomes_ml$metabolism_comment <- factor(training_outcomes_ml$metabolism_comment)
training_outcomes_ml$earliest_journal <- factor(training_outcomes_ml$earliest_journal)
training_outcomes_ml$mechanism_action_type_main <- factor(training_outcomes_ml$mechanism_action_type_main)
training_outcomes_ml$mechanism_action_type_parent_main <- factor(training_outcomes_ml$mechanism_action_type_parent_main)
training_outcomes_ml$target_type_main <- factor(training_outcomes_ml$target_type_main)
training_outcomes_ml$target_type_scd <- factor(training_outcomes_ml$target_type_scd)
training_outcomes_ml$target_type_thd <- factor(training_outcomes_ml$target_type_thd)
training_outcomes_ml$target_parent_type_main <- factor(training_outcomes_ml$target_parent_type_main)
training_outcomes_ml$highest_status <- factor(training_outcomes_ml$highest_status)
training_outcomes_ml$atc_l1_main <- factor(training_outcomes_ml$atc_l1_main)
training_outcomes_ml$atc_l1_mapped <- factor(training_outcomes_ml$atc_l1_mapped)
training_outcomes_ml$highest_status_indication <- factor(training_outcomes_ml$highest_status_indication) 

str(training_outcomes_ml)
summary(training_outcomes_ml)
```

# Dividing into training and test data sets: 

```{r, warning = FALSE}
set.seed(0802)

training_outcomes_ml_idx <- createDataPartition(training_outcomes_ml$highest_status,
p = 0.8, list = F)
training_outcomes_ml_train <- training_outcomes_ml[training_outcomes_ml_idx, ]
training_outcomes_ml_test <- training_outcomes_ml[-training_outcomes_ml_idx, ] 
```


# Random Forest

## Running the random forest model and calculating the assessment metrics: 

```{r, warning = FALSE}

rf_model <- randomForest(highest_status ~ ., data = training_outcomes_ml_train,
                  mtry = 10,
                  ntree = 1000)

predictions_rf <- predict(rf_model, newdata = training_outcomes_ml_test) 

cm_rf <- table(training_outcomes_ml_test$highest_status, predictions_rf) 
accuracy_rf <- sum(diag(cm_rf)) / sum(cm_rf) 
print(accuracy_rf) 

cm <- confusionMatrix(predictions_rf, training_outcomes_ml_test$highest_status) 
print(cm)

```



```{r, warning = FALSE}

num_cols <- ncol(training_outcomes_ml_train)
feature_names <- colnames(training_outcomes_ml_train[, - (num_cols - 3)])  # Replace 'training_outcomes_ml_train' with your data frame 

# Get the feature importance values
importance <- importance(rf_model)
# Create a data frame with feature names and their importance values
importance_df <- data.frame(Feature = feature_names, Importance = importance)

# Order the data frame by importance in descending order
ordered_importance_df <- importance_df[order(importance_df$MeanDecreaseGini, decreasing = TRUE), ]

# Print the ordered data frame
print(ordered_importance_df, width = 10000)

# Convert the Overall column to a numeric vector
ordered_importance_df$MeanDecreaseGini <- as.numeric(ordered_importance_df$MeanDecreaseGini)

barplot(ordered_importance_df$MeanDecreaseGini, names.arg = rownames(ordered_importance_df),
        xlab = "Features", ylab = "Importance", main = "Variable Importance")

# Create a data frame to store the class names and their respective importance values
class_importance_df <- data.frame(Class = rownames(ordered_importance_df),
                                  MeanDecreaseGini = as.numeric(ordered_importance_df[, "MeanDecreaseGini"]))

class_importance_df <- class_importance_df[order(class_importance_df$MeanDecreaseGini, decreasing = TRUE), ] 
class_importance_df$Class <- factor(class_importance_df$Class, levels = class_importance_df$Class) 

# Create a ggplot bar plot for feature importance for each class
p <- ggplot(class_importance_df, aes(x = Class, y = MeanDecreaseGini, fill = Class)) + 
  geom_bar(stat = "identity") +
  labs(x = "Input Features", y = "Importance", title = "Random Forest Feature Importance Plot") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "none")

# Print the plot 
print(p)

```


```{r, warning = FALSE}
print(rf_model$mtry)
print(rf_model$ntree)
```

```{r, warning = FALSE}
# Initialize vectors to store sensitivity and specificity for each class
sensitivity <- numeric(5)
specificity <- numeric(5)

# Calculate sensitivity (true positive rate) and specificity (true negative rate) for each class
for (i in 1:6) {
  TP <- cm$table[i, i]
  FN <- sum(cm$table[i, ]) - TP
  TN <- sum(diag(cm$table)) - sum(cm$table[i, ]) - sum(cm$table[, i]) + TP
  FP <- sum(cm$table[, i]) - TP

  sensitivity[i] <- TP / (TP + FN)
  specificity[i] <- TN / (TN + FP)
}

# Calculate balanced accuracy for each class (average of sensitivity and specificity)
balanced_accuracy <- (sensitivity + specificity) / 2

print(balanced_accuracy)

tp <- cm$table[row(cm$table) == col(cm$table)]
fp <- colSums(cm$table) - tp

# Calculate the overall precision
overall_precision <- sum(tp) / sum(tp + fp)

# Print the overall precision
print(paste("Overall Precision:", overall_precision))
```

```{r, warning = FALSE}
cm_rff <- table(training_outcomes_ml_test$highest_status, predictions_rf)
print(cm_rff)

# Calculate Precision for each class
precision <- diag(cm_rff) / colSums(cm_rff)
print(precision)

# Calculate Recall for each class
recall <- diag(cm_rff) / rowSums(cm_rff)
print(recall)

# Calculate Accuracy
accuracy <- sum(diag(cm_rff)) / sum(cm_rff)
print(accuracy)

# Calculate Balanced Accuracy
sensitivity <- diag(cm_rff) / colSums(cm_rff)
specificity <- rowSums(cm_rff) - diag(cm_rff) / (colSums(cm_rff) - 1)
balanced_accuracy <- mean((sensitivity + specificity) / 2)
print(balanced_accuracy)

# Calculate F1 Score for each class
f1_score <- 2 * precision * recall / (precision + recall)
print(f1_score)

```


# Neural Nets:

## Running the neural network model and calculating the assessment metrics: 

```{r, warning = FALSE}

nn_grid <- tuneGrid <- expand.grid(.size=c(5, 10, 15),
                        .decay=c(0.1, 0.01, 0.001))

nn_model <- train(
  highest_status ~ .,
  data = training_outcomes_ml_train,
  method = "nnet",
  tuneGrid = nn_grid
)

predictions_nn <- predict(nn_model, newdata = training_outcomes_ml_test)

dim(training_outcomes_ml_test)
dim(predictions_nn)

cm_nn <- confusionMatrix(predictions_nn, training_outcomes_ml_test$highest_status)
print(cm_nn) 
```

```{r, warning = FALSE}
# Retrieve the best performing hyperparameter values 
best_hyperparameters <- nn_model$bestTune
print(best_hyperparameters)
```

```{r, warning = FALSE}
cm_nn <- table(training_outcomes_ml_test$highest_status, predictions_nn)
print(cm_nn)

# Calculate Precision for each class
precision <- diag(cm_nn) / colSums(cm_nn)
print(precision)

# Calculate Recall for each class
recall <- diag(cm_nn) / rowSums(cm_nn)
print(recall)

# Calculate Accuracy
accuracy <- sum(diag(cm_nn)) / sum(cm_nn)
print(accuracy)

# Calculate Balanced Accuracy
sensitivity <- diag(cm_nn) / colSums(cm_nn)
specificity <- rowSums(cm_nn) - diag(cm_nn) / (colSums(cm_nn) - 1)
balanced_accuracy <- mean((sensitivity + specificity) / 2)
print(balanced_accuracy)

# Calculate F1 Score for each class
f1_score <- 2 * precision * recall / (precision + recall)
print(f1_score)

# Get predicted probabilities for each class
predicted_probabilities <- predict(nn_model, newdata = training_outcomes_ml_test, type = "prob")

# Calculate AUC-ROC for each class
roc_values <- lapply(1:6, function(i) {
  roc_obj <- multiclass.roc(training_outcomes_ml_test$highest_status, factor(predictions_nn, levels = c("Discontinued",
                                                                       
                                                                       "Pre Clinical Trials",
                                                                       
                                                                       "Phase 1 Clinical Trials",
                                                                       
                                                                       "Phase 2 Clinical Trials", 
                                                                       
                                                                       "Phase 3 Clinical Trials",
                                                                       
                                                                       "Approved"),
                                                    
                                                    , ordered = T))
})

# Print the AUC-ROC values for each class
print(roc_values)
```

# Install the required packages: 

```{r, warning = FALSE}
#install.packages("tidyverse")
#install.packages("Hmisc")
#install.packages("VIM")
#install.packages("lubridate")
#install.packages("stringr")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("rvest")
#install.packages("caret")
#install.packages("randomForest")
#install.packages("glmnet")
#install.packages("xgboost")
#install.packages("nnet")
#install.packages("rpart")
#install.packages("pROC")
#install.packages("rstatix")
#install.packages("ggpubr")
```


# Run the installed packages and set the standardization: 

```{r, warning = FALSE}
library(tidyverse)
library(Hmisc)
library(VIM)
library(lubridate)
library(stringr)
library(dplyr)
library(ggplot2)
library(rvest)
library(caret)
library(randomForest)
library(glmnet)
library(xgboost)
library(nnet)
library(rpart)
library(pROC)
library(rstatix)
library(ggpubr)
options(encoding = "utf-8")
```


# Import the data: 

```{r, warning = FALSE}
training_outcomes <- read.csv("C:/Users/0037R00002smXI0QAM/Desktop/ms_training_outcomes_ml_updated.csv", header = T)
```


# Visualize the highest status split: 

```{r, warning = FALSE}
training_outcomes$highest_status = factor(training_outcomes$highest_status, levels = c("Discontinued", "Pre Clinical Trials", "Phase 1 Clinical Trials", "Phase 2 Clinical Trials", "Phase 3 Clinical Trials", "Approved"))

ggplot(data = training_outcomes, aes(x = highest_status, fill = highest_status)) + 
    geom_bar() + 
    labs(title = "Highest status in drug development pipeline") +
    theme(plot.title = element_text(size = 20, face = "bold"),
          panel.background = element_rect(fill = "transparent"),
          panel.grid.major = element_blank(),                   #
          panel.grid.minor = element_blank()) +
    geom_text(stat = "count", aes(label = stat(count)), vjust = -0.5)
```


# Removing the string columns and other non-relevant columns while completing other data imputation steps (Ex: defining factor variables): 

```{r, warning = FALSE}
training_outcomes_ml <- training_outcomes[, -c(1, 2, 3, 4, 5, 9, 17, 20, 21, 23, 24)]

training_outcomes_ml$ro3_pass <- factor(training_outcomes_ml$ro3_pass)
training_outcomes_ml$molecular_species <- factor(training_outcomes_ml$molecular_species)
training_outcomes_ml$molecule_type <- factor(training_outcomes_ml$molecule_type)
training_outcomes_ml$metabolism_comment <- factor(training_outcomes_ml$metabolism_comment)
training_outcomes_ml$earliest_journal <- factor(training_outcomes_ml$earliest_journal)
training_outcomes_ml$mechanism_action_type_main <- factor(training_outcomes_ml$mechanism_action_type_main)
training_outcomes_ml$mechanism_action_type_parent_main <- factor(training_outcomes_ml$mechanism_action_type_parent_main)
training_outcomes_ml$target_type_main <- factor(training_outcomes_ml$target_type_main)
training_outcomes_ml$target_type_scd <- factor(training_outcomes_ml$target_type_scd)
training_outcomes_ml$target_type_thd <- factor(training_outcomes_ml$target_type_thd)
training_outcomes_ml$target_parent_type_main <- factor(training_outcomes_ml$target_parent_type_main)
training_outcomes_ml$highest_status <- factor(training_outcomes_ml$highest_status)
training_outcomes_ml$atc_l1_main <- factor(training_outcomes_ml$atc_l1_main)
training_outcomes_ml$atc_l1_mapped <- factor(training_outcomes_ml$atc_l1_mapped)
training_outcomes_ml$highest_status_indication <- factor(training_outcomes_ml$highest_status_indication) 

str(training_outcomes_ml)
summary(training_outcomes_ml)
```


# Dividing into training and test data sets: 

```{r, warning = FALSE}
set.seed(0802)

training_outcomes_ml_idx <- createDataPartition(training_outcomes_ml$highest_status,
p = 0.8, list = F)
training_outcomes_ml_train <- training_outcomes_ml[training_outcomes_ml_idx, ]
training_outcomes_ml_test <- training_outcomes_ml[-training_outcomes_ml_idx, ] 
```


# Random Forest

## Running the random forest model and calculating the assessment metrics: 

```{r, warning = FALSE}

rf_model <- randomForest(highest_status ~ ., data = training_outcomes_ml_train,
                  mtry = 10,
                  ntree = 1000)

predictions_rf <- predict(rf_model, newdata = training_outcomes_ml_test) 

cm_rf <- table(training_outcomes_ml_test$highest_status, predictions_rf) 
accuracy_rf <- sum(diag(cm_rf)) / sum(cm_rf) 
print(accuracy_rf) 

cm <- confusionMatrix(predictions_rf, training_outcomes_ml_test$highest_status) 
print(cm)

```



```{r, warning = FALSE}

num_cols <- ncol(training_outcomes_ml_train)
feature_names <- colnames(training_outcomes_ml_train[, - (num_cols - 3)])  # Replace 'training_outcomes_ml_train' with your data frame 

# Get the feature importance values
importance <- importance(rf_model)
# Create a data frame with feature names and their importance values
importance_df <- data.frame(Feature = feature_names, Importance = importance)

# Order the data frame by importance in descending order
ordered_importance_df <- importance_df[order(importance_df$MeanDecreaseGini, decreasing = TRUE), ]

# Print the ordered data frame
print(ordered_importance_df, width = 10000)

# Convert the Overall column to a numeric vector
ordered_importance_df$MeanDecreaseGini <- as.numeric(ordered_importance_df$MeanDecreaseGini)

barplot(ordered_importance_df$MeanDecreaseGini, names.arg = rownames(ordered_importance_df),
        xlab = "Features", ylab = "Importance", main = "Variable Importance")

# Create a data frame to store the class names and their respective importance values
class_importance_df <- data.frame(Class = rownames(ordered_importance_df),
                                  MeanDecreaseGini = as.numeric(ordered_importance_df[, "MeanDecreaseGini"]))

class_importance_df <- class_importance_df[order(class_importance_df$MeanDecreaseGini, decreasing = TRUE), ] 
class_importance_df$Class <- factor(class_importance_df$Class, levels = class_importance_df$Class) 

# Create a ggplot bar plot for feature importance for each class
p <- ggplot(class_importance_df, aes(x = Class, y = MeanDecreaseGini, fill = Class)) + 
  geom_bar(stat = "identity") +
  labs(x = "Input Features", y = "Importance", title = "Random Forest Feature Importance Plot (Based on LASSO Regression Feature Selection)") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "none") 

# Print the plot 
print(p) 

```


```{r, warning = FALSE}
print(rf_model$mtry)
print(rf_model$ntree)
```

```{r, warning = FALSE}
# Initialize vectors to store sensitivity and specificity for each class
sensitivity <- numeric(5)
specificity <- numeric(5)

# Calculate sensitivity (true positive rate) and specificity (true negative rate) for each class
for (i in 1:6) {
  TP <- cm$table[i, i]
  FN <- sum(cm$table[i, ]) - TP
  TN <- sum(diag(cm$table)) - sum(cm$table[i, ]) - sum(cm$table[, i]) + TP
  FP <- sum(cm$table[, i]) - TP

  sensitivity[i] <- TP / (TP + FN)
  specificity[i] <- TN / (TN + FP)
}

# Calculate balanced accuracy for each class (average of sensitivity and specificity)
balanced_accuracy <- (sensitivity + specificity) / 2

print(balanced_accuracy)

tp <- cm$table[row(cm$table) == col(cm$table)]
fp <- colSums(cm$table) - tp

# Calculate the overall precision
overall_precision <- sum(tp) / sum(tp + fp)

# Print the overall precision
print(paste("Overall Precision:", overall_precision))
```

```{r, warning = FALSE}
cm_rff <- table(training_outcomes_ml_test$highest_status, predictions_rf)
print(cm_rff)

# Calculate Precision for each class
precision <- diag(cm_rff) / colSums(cm_rff)
print(precision)

# Calculate Recall for each class
recall <- diag(cm_rff) / rowSums(cm_rff)
print(recall)

# Calculate Accuracy
accuracy <- sum(diag(cm_rff)) / sum(cm_rff)
print(accuracy)

# Calculate Balanced Accuracy
sensitivity <- diag(cm_rff) / colSums(cm_rff)
specificity <- rowSums(cm_rff) - diag(cm_rff) / (colSums(cm_rff) - 1)
balanced_accuracy <- mean((sensitivity + specificity) / 2)
print(balanced_accuracy)

# Calculate F1 Score for each class
f1_score <- 2 * precision * recall / (precision + recall)
print(f1_score)

# Calculate AUC-ROC for each class
roc_values_rf <- lapply(1:6, function(i) {
  roc_obj_rf <- multiclass.roc(training_outcomes_ml_test$highest_status, factor(predictions_rf, levels = c("Discontinued",
                                                                       
                                                                       "Pre Clinical Trials",
                                                                       
                                                                       "Phase 1 Clinical Trials",
                                                                       
                                                                       "Phase 2 Clinical Trials", 
                                                                       
                                                                       "Phase 3 Clinical Trials",
                                                                       
                                                                       "Approved"),
                                                    
                                                    , ordered = T))
})

# Print the AUC-ROC values for each class
print(roc_values_rf)

```


# Neural Nets:

## Running the neural network model and calculating the assessment metrics: 

```{r, warning = FALSE}

nn_grid <- tuneGrid <- expand.grid(.size=c(5, 10, 15),
                        .decay=c(0.1, 0.01, 0.001))

nn_model <- train(
  highest_status ~ .,
  data = training_outcomes_ml_train,
  method = "nnet",
  tuneGrid = nn_grid,
  reproducible = TRUE
)

predictions_nn <- predict(nn_model, newdata = training_outcomes_ml_test)

dim(training_outcomes_ml_test)
dim(predictions_nn)

cm_nn <- confusionMatrix(predictions_nn, training_outcomes_ml_test$highest_status)
print(cm_nn) 
```

```{r, warning = FALSE}
# Retrieve the best performing hyperparameter values 
best_hyperparameters <- nn_model$bestTune
print(best_hyperparameters)
```


```{r, warning = FALSE}
cm_nn <- table(training_outcomes_ml_test$highest_status, predictions_nn)
print(cm_nn)

# Calculate Precision for each class
precision <- diag(cm_nn) / colSums(cm_nn)
print(precision)

# Calculate Recall for each class
recall <- diag(cm_nn) / rowSums(cm_nn)
print(recall)

# Calculate Accuracy
accuracy <- sum(diag(cm_nn)) / sum(cm_nn)
print(accuracy)

# Calculate Balanced Accuracy
sensitivity <- diag(cm_nn) / colSums(cm_nn)
specificity <- rowSums(cm_nn) - diag(cm_nn) / (colSums(cm_nn) - 1)
balanced_accuracy <- mean((sensitivity + specificity) / 2)
print(balanced_accuracy)

# Calculate F1 Score for each class
f1_score <- 2 * precision * recall / (precision + recall)
print(f1_score)

# Get predicted probabilities for each class
predicted_probabilities <- predict(nn_model, newdata = training_outcomes_ml_test, type = "prob")

# Calculate AUC-ROC for each class
roc_values <- lapply(1:6, function(i) {
  roc_obj <- multiclass.roc(training_outcomes_ml_test$highest_status, factor(predictions_nn, levels = c("Discontinued",
                                                                       
                                                                       "Pre Clinical Trials",
                                                                       
                                                                       "Phase 1 Clinical Trials",
                                                                       
                                                                       "Phase 2 Clinical Trials", 
                                                                       
                                                                       "Phase 3 Clinical Trials",
                                                                       
                                                                       "Approved"),
                                                    
                                                    , ordered = T))
})

# Print the AUC-ROC values for each class
print(roc_values)

```


# Install the required packages: 

```{r, warning = FALSE}
#install.packages("tidyverse")
#install.packages("Hmisc")
#install.packages("VIM")
#install.packages("lubridate")
#install.packages("stringr")
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("rvest")
#install.packages("caret")
#install.packages("randomForest")
#install.packages("glmnet")
#install.packages("xgboost")
#install.packages("nnet")
#install.packages("rpart")
#install.packages("pROC")
#install.packages("rstatix")
#install.packages("ggpubr")
```


# Run the installed packages and set the standardization: 

```{r, warning = FALSE}
library(tidyverse)
library(Hmisc)
library(VIM)
library(lubridate)
library(stringr)
library(dplyr)
library(ggplot2)
library(rvest)
library(caret)
library(randomForest)
library(glmnet)
library(xgboost)
library(nnet)
library(rpart)
library(pROC)
library(rstatix)
library(ggpubr)
options(encoding = "utf-8")
```


# Import the data: 

```{r, warning = FALSE}
training_outcomes <- read.csv("C:/Users/0037R00002smXI0QAM/Desktop/ms_training_outcomes_ml_updated.csv", header = T)
```


# Visualize the highest status split: 

```{r, warning = FALSE}
training_outcomes$highest_status = factor(training_outcomes$highest_status, levels = c("Discontinued", "Pre Clinical Trials", "Phase 1 Clinical Trials", "Phase 2 Clinical Trials", "Phase 3 Clinical Trials", "Approved"))

ggplot(data = training_outcomes, aes(x = highest_status, fill = highest_status)) + 
    geom_bar() + 
    labs(title = "Highest status in drug development pipeline") +
    theme(plot.title = element_text(size = 20, face = "bold"),
          panel.background = element_rect(fill = "transparent"),
          panel.grid.major = element_blank(),                   #
          panel.grid.minor = element_blank()) +
    geom_text(stat = "count", aes(label = stat(count)), vjust = -0.5)
```


# Removing the string columns and other non-relevant columns while completing other data imputation steps (Ex: defining factor variables): 

```{r, warning = FALSE}
training_outcomes_ml <- training_outcomes[, -c(1, 2, 3, 20, 21, 23, 24, 25, 16, 12, 22, 18, 8, 28, 11, 32, 36, 27, 29, 19, 26, 30, 31, 10, 4, 5)]

training_outcomes_ml$highest_status <- factor(training_outcomes_ml$highest_status)
training_outcomes_ml$atc_l1_main <- factor(training_outcomes_ml$atc_l1_main)
training_outcomes_ml$atc_l1_mapped <- factor(training_outcomes_ml$atc_l1_mapped)

str(training_outcomes_ml)
summary(training_outcomes_ml)
```


# Dividing into training and test data sets: 

```{r, warning = FALSE}
set.seed(0802)

training_outcomes_ml_idx <- createDataPartition(training_outcomes_ml$highest_status,
p = 0.8, list = F)
training_outcomes_ml_train <- training_outcomes_ml[training_outcomes_ml_idx, ]
training_outcomes_ml_test <- training_outcomes_ml[-training_outcomes_ml_idx, ] 
```


# Random Forest

## Running the random forest model and calculating the assessment metrics: 

```{r, warning = FALSE}

rf_model <- randomForest(highest_status ~ ., data = training_outcomes_ml_train,
                  mtry = 10,
                  ntree = 1000)

predictions_rf <- predict(rf_model, newdata = training_outcomes_ml_test) 

cm_rf <- table(training_outcomes_ml_test$highest_status, predictions_rf) 
accuracy_rf <- sum(diag(cm_rf)) / sum(cm_rf) 
print(accuracy_rf) 

cm <- confusionMatrix(predictions_rf, training_outcomes_ml_test$highest_status) 
print(cm)

```


```{r, warning = FALSE}

num_cols <- ncol(training_outcomes_ml_train)
feature_names <- colnames(training_outcomes_ml_train[, - (num_cols - 3)])  # Replace 'training_outcomes_ml_train' with your data frame 

# Get the feature importance values
importance <- importance(rf_model)
# Create a data frame with feature names and their importance values
importance_df <- data.frame(Feature = feature_names, Importance = importance)

# Order the data frame by importance in descending order
ordered_importance_df <- importance_df[order(importance_df$MeanDecreaseGini, decreasing = TRUE), ]

# Print the ordered data frame
print(ordered_importance_df, width = 10000)

# Convert the Overall column to a numeric vector
ordered_importance_df$MeanDecreaseGini <- as.numeric(ordered_importance_df$MeanDecreaseGini)

barplot(ordered_importance_df$MeanDecreaseGini, names.arg = rownames(ordered_importance_df),
        xlab = "Features", ylab = "Importance", main = "Variable Importance")

# Create a data frame to store the class names and their respective importance values
class_importance_df <- data.frame(Class = rownames(ordered_importance_df),
                                  MeanDecreaseGini = as.numeric(ordered_importance_df[, "MeanDecreaseGini"]))

class_importance_df <- class_importance_df[order(class_importance_df$MeanDecreaseGini, decreasing = TRUE), ] 
class_importance_df$Class <- factor(class_importance_df$Class, levels = class_importance_df$Class) 

# Create a ggplot bar plot for feature importance for each class
p <- ggplot(class_importance_df, aes(x = Class, y = MeanDecreaseGini, fill = Class)) + 
  geom_bar(stat = "identity") +
  labs(x = "Input Features", y = "Importance", title = "Random Forest Feature Importance Plot (Based on RF Feature Importance)") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "none")

# Print the plot 
print(p) 

```


```{r, warning = FALSE}
cm_rff <- table(training_outcomes_ml_test$highest_status, predictions_rf)
print(cm_rff)

# Calculate Precision for each class
precision <- diag(cm_rff) / colSums(cm_rff)
print(precision)

# Calculate Recall for each class
recall <- diag(cm_rff) / rowSums(cm_rff)
print(recall)

# Calculate Accuracy
accuracy <- sum(diag(cm_rff)) / sum(cm_rff)
print(accuracy)

# Calculate Balanced Accuracy
sensitivity <- diag(cm_rff) / colSums(cm_rff)
specificity <- rowSums(cm_rff) - diag(cm_rff) / (colSums(cm_rff) - 1)
balanced_accuracy <- mean((sensitivity + specificity) / 2)
print(balanced_accuracy)

# Calculate F1 Score for each class
f1_score <- 2 * precision * recall / (precision + recall)
print(f1_score)

# Calculate AUC-ROC for each class
roc_values_rf <- lapply(1:6, function(i) {
  roc_obj_rf <- multiclass.roc(training_outcomes_ml_test$highest_status, factor(predictions_rf, levels = c("Discontinued",
                                                                       
                                                                       "Pre Clinical Trials",
                                                                       
                                                                       "Phase 1 Clinical Trials",
                                                                       
                                                                       "Phase 2 Clinical Trials", 
                                                                       
                                                                       "Phase 3 Clinical Trials",
                                                                       
                                                                       "Approved"),
                                                    
                                                    , ordered = T))
})

# Print the AUC-ROC values for each class
print(roc_values_rf)

```


# Neural Nets:

## Running the neural netwrok model and calculating the assessment metrics: 

```{r, warning = FALSE}

nn_grid <- tuneGrid <- expand.grid(.size=c(5, 10, 15),
                        .decay=c(0.1, 0.01, 0.001))

nn_model <- train(
  highest_status ~ .,
  data = training_outcomes_ml_train,
  method = "nnet",
  tuneGrid = nn_grid
)

predictions_nn <- predict(nn_model, newdata = training_outcomes_ml_test)

dim(training_outcomes_ml_test)
dim(predictions_nn)

cm_nn <- confusionMatrix(predictions_nn, training_outcomes_ml_test$highest_status)
print(cm_nn) 
```

```{r, warning = FALSE}
# Retrieve the best performing hyperparameter values 
best_hyperparameters <- nn_model$bestTune
print(best_hyperparameters)
```

```{r, warning = FALSE}
cm_nn <- table(training_outcomes_ml_test$highest_status, predictions_nn)
print(cm_nn)

# Calculate Precision for each class
precision <- diag(cm_nn) / colSums(cm_nn)
print(precision)

# Calculate Recall for each class
recall <- diag(cm_nn) / rowSums(cm_nn)
print(recall)

# Calculate Accuracy
accuracy <- sum(diag(cm_nn)) / sum(cm_nn)
print(accuracy)

# Calculate Balanced Accuracy
sensitivity <- diag(cm_nn) / colSums(cm_nn)
specificity <- rowSums(cm_nn) - diag(cm_nn) / (colSums(cm_nn) - 1)
balanced_accuracy <- mean((sensitivity + specificity) / 2)
print(balanced_accuracy)

# Calculate F1 Score for each class
f1_score <- 2 * precision * recall / (precision + recall)
print(f1_score)

# Get predicted probabilities for each class
predicted_probabilities <- predict(nn_model, newdata = training_outcomes_ml_test, type = "prob")

# Calculate AUC-ROC for each class
roc_values <- lapply(1:6, function(i) {
  roc_obj <- multiclass.roc(training_outcomes_ml_test$highest_status, factor(predictions_nn, levels = c("Discontinued",
                                                                       
                                                                       "Pre Clinical Trials",
                                                                       
                                                                       "Phase 1 Clinical Trials",
                                                                       
                                                                       "Phase 2 Clinical Trials", 
                                                                       
                                                                       "Phase 3 Clinical Trials",
                                                                       
                                                                       "Approved"),
                                                    
                                                    , ordered = T))
})

# Print the AUC-ROC values for each class
print(roc_values)
```



